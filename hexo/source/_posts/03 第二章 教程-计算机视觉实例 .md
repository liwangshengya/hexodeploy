---
title: 第二章计算机视觉
date: 2022-02-11 22:15:15
tags:
    - tensorflow
categories: tensorflow
copyright: false
---
注：该分类下的文章教程部分均来自MOOC的([TensorFlow 入门实操课程_网易有道_中国大学MOOC(慕课) (icourse163.org)](https://www.icourse163.org/course/youdao-1460578162)),文章最后的练习部分是自己写的

##  超越Hello World，一个计算机视觉的实例

在上一个练习中，你看到了如何创建一个神经网络，让它学会图解决某个问题。那是一个计算机习得行为的明确例子。当然，就那个例子而言，运用神经元网络来解决有点大炮轰苍蝇的意思，因为直接写出函数Y=2x-1会更容易，而不是费力地使用机器学习来学习X和Y之间对于一组固定值的关系，并将其扩展到所有值。

但是，在一个场景下，写出明确的函数规则要困难得多，比如计算机视觉问题。让我们来看这样一个场景，让计算机识别不同的服装用品（有提包、鞋子、裤子等10类物品）。我们将用包含10种不同类型的物品图片的数据集来训练一个神经元网络，实现分类。

## 开始编程

首先导入TensorFlow


```python
import tensorflow as tf
print(tf.__version__)
```

    2.2.0
    通过tf.keras的API可以直接获得Fashion MNIST数据集，像这样：


```python
mnist = tf.keras.datasets.fashion_mnist
在mnist对象上调用load_data方法会得到两个元组，各自包含两个列表。这些列表存储了服装用品的训练与测试图像数据及标签值。
```


```python
(training_images, training_labels), (test_images, test_labels) = mnist.load_data()
这些值是什么样子的呢？让我们打印一个训练图像，和一个训练标签来看看......用数组中不同的索引做实验。例如，也可以看看索引42......那也是另一张鞋子的图片。
```


```python
import matplotlib.pyplot as plt
plt.imshow(training_images[0])
print(training_labels[0])
print(training_images[0])
```

    9
    [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
        0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
        0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
        0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0
        0   1   4   0   0   0   0   1   1   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62
       54   0   0   0   1   3   4   0   0   3]
     [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134
      144 123  23   0   0   0   0  12  10   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178
      107 156 161 109  64  23  77 130  72  15]
     [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216
      216 163 127 121 122 146 141  88 172  66]
     [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229
      223 223 215 213 164 127 123 196 229   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228
      235 227 224 222 224 221 223 245 173   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198
      180 212 210 211 213 223 220 243 202   0]
     [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192
      169 227 208 218 224 212 226 197 209  52]
     [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203
      198 221 215 213 222 220 245 119 167  56]
     [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240
      232 213 218 223 234 217 217 209  92   0]
     [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219
      222 221 216 223 229 215 218 255  77   0]
     [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208
      211 218 224 223 219 215 224 244 159   0]
     [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230
      224 234 176 188 250 248 233 238 215   0]
     [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223
      255 255 221 234 221 211 220 232 246   0]
     [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221
      188 154 191 210 204 209 222 228 225   0]
     [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117
      168 219 221 215 217 223 223 224 229  29]
     [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245
      239 223 218 212 209 222 220 221 230  67]
     [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216
      199 206 186 181 177 172 181 205 206 115]
     [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191
      195 191 198 192 176 156 167 177 210  92]
     [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209
      210 210 211 188 188 194 192 216 170   0]
     [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179
      182 182 181 176 166 168  99  58   0   0]
     [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0
        0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
        0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
        0   0   0   0   0   0   0   0   0   0]]




![png](output_8_1.png)
    

你会注意到，数字中的所有值都在0和255之间。如果我们要训练一个神经网络，出于多种原因，如果把所有的值都处理成0和1之间，那就更容易得到较好的训练效果。这个过程叫做 "**归一化**"......幸运的是，在Python中，很容易对这样的列表进行归一化，而不需要循环。可以这样做：


```python
training_images  = training_images / 255.0
test_images = test_images / 255.0
```

你可能在想为什么有2组数据-训练集和测试集。记得在介绍中说过的吗？基本想法是将1组数据用于训练，然后用另一组数据评估模型在分类值方面的表现会有多好。测试数据必须是模型还没有看到过的。毕竟，当完成模型的训练后，必定想用它之前没有见过的数据来试一试!
现在我们来设计Fashion MNIST分类模型。这里有不少新的概念，不过别担心，你会掌握它们的窍门。


```python
model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), 
                                    tf.keras.layers.Dense(128, activation=tf.nn.relu), 
                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])
```

**Sequential**。这定义了神经网络中的层数序列。一开始学习神经元网络总是使用序列模型。

**Flatten**。还记得上面将图像打印出来的时候是一个正方形吗？扁平化只是把这个正方形变成了一个一维的集合。把二维数组变成一维数组。

**Dense**：增加一层神经元。

每一层神经元都需要一个**激活函数 activation**来告诉它们输出什么。有很多选项，但目前只用这些（relu和softmax）。

**Relu**的意思是 "如果X>0返回X，否则返回0"--所以它的作用是它只把大于0的值传递给网络中的下一层，小于0的也当作0。

**Softmax**激活函数接收到一组值后，选择其中最大的一个输出。例如，上一层的输出为[0.1, 0.1, 0.05, 0.1, 9.5, 0.1, 0.05, 0.05, 0.05]，Softmax就省去了你在其中寻找最大的值，并把它变成[0,0,0,0,0,1,0,0,0,0,0] **Softmax**的意思是 "如果X>0，则返回X，否则返回0" -- 所以它的作用是只把0或更大的值传给下一层的网络。--其目的是节省大量的编码！. 

现在模型已经定义好了，下一步要做的事情就是实际建立它。可以像之前一样用优化器和损失函数编译它--然后通过调用*model.fit*来训练它，要求它将训练数据与标签拟合--即让模型找出训练数据和标签之间的关系。训练好之后，它将能对格式与训练数据相同，但从未“见过”的新数据做出预测。


```python
model.compile(optimizer = tf.optimizers.Adam(),
              loss = 'sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(training_images, training_labels, epochs=5)
```

    Epoch 1/5
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.5024 - accuracy: 0.8238
    Epoch 2/5
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.3796 - accuracy: 0.8625
    Epoch 3/5
    1875/1875 [==============================] - 3s 1ms/step - loss: 0.3406 - accuracy: 0.8756
    Epoch 4/5
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.3162 - accuracy: 0.8840
    Epoch 5/5
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.2967 - accuracy: 0.8902





    <tensorflow.python.keras.callbacks.History at 0x7fb209d45290>

一旦模型完成了训练 -- 能在最后一个epoch结束时看到一个准确率值。它可能看起来像0.9098。这告诉我们，神经网络对训练数据的分类准确率约为91%。即，它找出了图像和标签之间的模式匹配，91%的分类结果都正确。考虑到只训练了5个epochs，而且做得相当快，所以结果还不错。

但对于未见过的数据，它的分类准确度有多高？这就是为什么我们需要测试图像的原因。我们可以调用model.evaluation，并将用于测试的图像和标签数据传入，它会报告测试数据的损失。让我们试一试。


```python
model.evaluate(test_images, test_labels)
```

    313/313 [==============================] - 0s 996us/step - loss: 0.3663 - accuracy: 0.8699
    
    [0.3662574589252472, 0.8698999881744385]

模型评估返回的准确率约为0.8838，这意味着它的准确率约为88%。正如预期的那样，模型对*未见过*的数据可能不会像对它训练过的数据那样做得那么好!  在这门课程中，我们会学习如何提高模型对测试数据的准确率。

请尝试下面的练习，进一步探索。

### 练习1

请运行以下代码。程序根据测试图像集生成了一组分类，然后打印出分类中的第一条记录。 运行print后，输出的是一个数字列表。请解释下这些数字代表什么？


```python
classifications = model.predict(test_images)

print(classifications[0])
```

    [3.3608790e-06 2.9126397e-08 2.9933884e-07 2.8684832e-07 1.5305069e-07
     1.9125285e-02 2.0043708e-06 7.6962374e-03 9.5409378e-06 9.7316283e-01]

提示：试着运行print(test_labels[0]) -- 会得到一个9。这是否有助于理解为什么列表看起来是这样的？


```python
print(test_labels[0])
```

    9

#### 这个列表代表什么？

1.   这是10个随机的无意义的值
2.   这是计算机做出的前10张图片的分类。
3.   这是一张图像对应10类中每一类的概率。
     正确答案是(3)

该模型的输出是一个由10个数字组成的列表。这些数字是被分类的值与对应值的概率，即列表中的第一个值是类别 "0 "的概率，下一个是 "1 "的概率，等等。注意，它们的概率都很低。

对于9，概率最高，即神经网络告诉我们，最可能的是9。

#### 怎么知道这个列表告诉你这个商品是踝靴？


1.   没有足够的信息来回答这个问题。
2.   列表中的第10个元素是最大的，并且踝靴的标签是9。
2.   踝靴的标签为9，列表中存在0->9个元素。

##### 回答

正确答案是（2）。列表和标签都是以0为基础的，所以脚踝靴的标签为9，意味着它是10个类别中的第10个。列表中第10个元素的值最高，意味着神经网络已经预测出它所分类的物品最有可能是一只脚踝靴。

### 练习2

现在让我们看看模型的中间层。尝试用不同值代替512（神经元），对全连接层进行实验。得到的损失、训练时间等结果有什么不同？为什么会这样？


```python
import tensorflow as tf
print(tf.__version__)

mnist = tf.keras.datasets.mnist

(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()

training_images = training_images/255.0
test_images = test_images/255.0

model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(512, activation=tf.nn.relu),
                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

model.compile(optimizer = 'adam',
              loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])

model.fit(training_images, training_labels, epochs=5)

model.evaluate(test_images, test_labels)

classifications = model.predict(test_images)

print(classifications[0])
print(test_labels[0])
```

    2.2.0
    Epoch 1/5
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.1996 - accuracy: 0.9415
    Epoch 2/5
    1875/1875 [==============================] - 4s 2ms/step - loss: 0.0790 - accuracy: 0.9764
    Epoch 3/5
    1875/1875 [==============================] - 4s 2ms/step - loss: 0.0529 - accuracy: 0.9833
    Epoch 4/5
    1875/1875 [==============================] - 4s 2ms/step - loss: 0.0376 - accuracy: 0.9878
    Epoch 5/5
    1875/1875 [==============================] - 4s 2ms/step - loss: 0.0261 - accuracy: 0.9912
    313/313 [==============================] - 0s 1ms/step - loss: 0.0768 - accuracy: 0.9776
    [4.4536002e-09 2.1431958e-09 1.4628866e-07 2.4208452e-05 4.9488766e-13
     4.5130352e-07 7.8295416e-14 9.9996972e-01 1.7931168e-07 5.2998671e-06]
    7

#### 问题1：增加到1024个神经元--有什么影响？

1. 训练时间更长，但更准确
2. 训练时间较长，但对准确性没有影响
3. 训练需要同样的时间，但更准确。

##### 回答

正确的答案是(1)通过增加更多的神经元，计算机必须做更多的计算，减慢了训练过程。但对于本案例，增加神经元数量有积极的影响--确实得到了更好的准确度。但这并不意味着总是 "越多越好"，因为很快就会遇到收益递减的定律。

### 练习3

如果删除Flatten()层，会发生什么情况？为什么会出现这种情况？

会得到一个关于数据形状的错误。因为神经元网络中的第一层应该与输入数据的形状相同。现在我们的数据是28x28的图像，28层28个神经元是不可行的，所以把28,28 "扁平化 "成784x1。不需要自己编写所有的代码来实现扁平化，而是在模型最上面添加Flatten()层，当把数组加载到模型中时，它们会自动为扁平化。


```python
import tensorflow as tf
print(tf.__version__)

mnist = tf.keras.datasets.mnist

(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()

training_images = training_images/255.0
test_images = test_images/255.0

model = tf.keras.models.Sequential([#tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),
                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

model.compile(optimizer = 'adam',
              loss = 'sparse_categorical_crossentropy')

model.fit(training_images, training_labels, epochs=5)

model.evaluate(test_images, test_labels)

classifications = model.predict(test_images)

print(classifications[0])
print(test_labels[0])
```

    2.2.0
    Epoch 1/5



    ---------------------------------------------------------------------------
    
    ValueError                                Traceback (most recent call last)
    
    <ipython-input-18-cd0411f1d295> in <module>
         16               loss = 'sparse_categorical_crossentropy')
         17 
    ---> 18 model.fit(training_images, training_labels, epochs=5)
         19 
         20 model.evaluate(test_images, test_labels)


    /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
         64   def _method_wrapper(self, *args, **kwargs):
         65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
    ---> 66       return method(self, *args, **kwargs)
         67 
         68     # Running inside `run_distribute_coordinator` already.


    /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
        846                 batch_size=batch_size):
        847               callbacks.on_train_batch_begin(step)
    --> 848               tmp_logs = train_function(iterator)
        849               # Catch OutOfRangeError for Datasets of unknown size.
        850               # This blocks until the batch has finished executing.


    /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
        578         xla_context.Exit()
        579     else:
    --> 580       result = self._call(*args, **kwds)
        581 
        582     if tracing_count == self._get_tracing_count():


    /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
        625       # This is the first call of __call__, so we have to initialize.
        626       initializers = []
    --> 627       self._initialize(args, kwds, add_initializers_to=initializers)
        628     finally:
        629       # At this point we know that the initialization is complete (or less


    /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
        504     self._concrete_stateful_fn = (
        505         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
    --> 506             *args, **kwds))
        507 
        508     def invalid_creator_scope(*unused_args, **unused_kwds):


    /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
       2444       args, kwargs = None, None
       2445     with self._lock:
    -> 2446       graph_function, _, _ = self._maybe_define_function(args, kwargs)
       2447     return graph_function
       2448 


    /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
       2775 
       2776       self._function_cache.missed.add(call_context_key)
    -> 2777       graph_function = self._create_graph_function(args, kwargs)
       2778       self._function_cache.primary[cache_key] = graph_function
       2779       return graph_function, args, kwargs


    /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
       2665             arg_names=arg_names,
       2666             override_flat_arg_shapes=override_flat_arg_shapes,
    -> 2667             capture_by_value=self._capture_by_value),
       2668         self._function_attributes,
       2669         # Tell the ConcreteFunction to clean up its graph once it goes out of


    /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
        979         _, original_func = tf_decorator.unwrap(python_func)
        980 
    --> 981       func_outputs = python_func(*func_args, **func_kwargs)
        982 
        983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,


    /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
        439         # __wrapped__ allows AutoGraph to swap in a converted function. We give
        440         # the function a weak reference to itself to avoid a reference cycle.
    --> 441         return weak_wrapped_fn().__wrapped__(*args, **kwds)
        442     weak_wrapped_fn = weakref.ref(wrapped_fn)
        443 


    /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
        966           except Exception as e:  # pylint:disable=broad-except
        967             if hasattr(e, "ag_error_metadata"):
    --> 968               raise e.ag_error_metadata.to_exception(e)
        969             else:
        970               raise


    ValueError: in user code:
    
        /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
            outputs = self.distribute_strategy.run(
        /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
            return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
        /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
            return self._call_for_each_replica(fn, args, kwargs)
        /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
            return fn(*args, **kwargs)
        /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:533 train_step  **
            y, y_pred, sample_weight, regularization_losses=self.losses)
        /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__
            loss_value = loss_obj(y_t, y_p, sample_weight=sw)
        /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:143 __call__
            losses = self.call(y_true, y_pred)
        /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:246 call
            return self.fn(y_true, y_pred, **self._fn_kwargs)
        /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:1558 sparse_categorical_crossentropy
            y_true, y_pred, from_logits=from_logits, axis=axis)
        /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:4655 sparse_categorical_crossentropy
            labels=target, logits=output)
        /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py:3591 sparse_softmax_cross_entropy_with_logits_v2
            labels=labels, logits=logits, name=name)
        /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py:3507 sparse_softmax_cross_entropy_with_logits
            logits.get_shape()))
    
        ValueError: Shape mismatch: The shape of labels (received (32, 1)) should equal the shape of logits except for the last dimension (received (32, 28, 10)).

### 练习4

考虑最后（产出）层。为什么有10个神经元？如果数量少于10会发生什么？例如，尝试改作5个来训练网络。

一旦模型发现一个意外的值，就会产生一个错误。规则是--最后一层的神经元数量应该与你要分类的类数相匹配。在这种情况下，是数字0-9，所以有10个，因此你的最后一层应该有10个神经元。


```python
import tensorflow as tf
print(tf.__version__)

mnist = tf.keras.datasets.mnist

(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()

training_images = training_images/255.0
test_images = test_images/255.0

model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),
                                    tf.keras.layers.Dense(5, activation=tf.nn.softmax)])

model.compile(optimizer = 'adam',
              loss = 'sparse_categorical_crossentropy')

model.fit(training_images, training_labels, epochs=5)

model.evaluate(test_images, test_labels)

classifications = model.predict(test_images)

print(classifications[0])
print(test_labels[0])
```

    2.2.0
    Epoch 1/5



    ---------------------------------------------------------------------------
    
    InvalidArgumentError                      Traceback (most recent call last)
    
    <ipython-input-19-04e5ea87fa1d> in <module>
         16               loss = 'sparse_categorical_crossentropy')
         17 
    ---> 18 model.fit(training_images, training_labels, epochs=5)
         19 
         20 model.evaluate(test_images, test_labels)


    /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
         64   def _method_wrapper(self, *args, **kwargs):
         65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
    ---> 66       return method(self, *args, **kwargs)
         67 
         68     # Running inside `run_distribute_coordinator` already.


    /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
        846                 batch_size=batch_size):
        847               callbacks.on_train_batch_begin(step)
    --> 848               tmp_logs = train_function(iterator)
        849               # Catch OutOfRangeError for Datasets of unknown size.
        850               # This blocks until the batch has finished executing.


    /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
        578         xla_context.Exit()
        579     else:
    --> 580       result = self._call(*args, **kwds)
        581 
        582     if tracing_count == self._get_tracing_count():


    /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
        642         # Lifting succeeded, so variables are initialized and we can run the
        643         # stateless function.
    --> 644         return self._stateless_fn(*args, **kwds)
        645     else:
        646       canon_args, canon_kwds = \


    /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
       2418     with self._lock:
       2419       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
    -> 2420     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
       2421 
       2422   @property


    /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs)
       1663          if isinstance(t, (ops.Tensor,
       1664                            resource_variable_ops.BaseResourceVariable))),
    -> 1665         self.captured_inputs)
       1666 
       1667   def _call_flat(self, args, captured_inputs, cancellation_manager=None):


    /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
       1744       # No tape is watching; skip to running the function.
       1745       return self._build_call_outputs(self._inference_function.call(
    -> 1746           ctx, args, cancellation_manager=cancellation_manager))
       1747     forward_backward = self._select_forward_and_backward_functions(
       1748         args,


    /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)
        596               inputs=args,
        597               attrs=attrs,
    --> 598               ctx=ctx)
        599         else:
        600           outputs = execute.execute_with_cancellation(


    /opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
         58     ctx.ensure_initialized()
         59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
    ---> 60                                         inputs, attrs, num_outputs)
         61   except core._NotOkStatusException as e:
         62     if name is not None:


    InvalidArgumentError:  Received a label value of 9 which is outside the valid range of [0, 5).  Label values: 8 3 8 9 6 6 0 5 7 6 2 7 8 2 1 7 9 3 3 6 0 0 2 1 8 7 4 6 5 3 0 0
    	 [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at <ipython-input-19-04e5ea87fa1d>:18) ]] [Op:__inference_train_function_31729]
    
    Function call stack:
    train_function

### 练习5

考虑网络中增加层数的影响。如果在512层和10层之间再加一层会发生什么？

答案：没有显著影响--因为这是相对简单的数据。对于复杂得多的数据，通常要增加额外的层。


```python
import tensorflow as tf
print(tf.__version__)

mnist = tf.keras.datasets.mnist

(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()

training_images = training_images/255.0
test_images = test_images/255.0

model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(512, activation=tf.nn.relu),
                                    tf.keras.layers.Dense(256, activation=tf.nn.relu),
                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

model.compile(optimizer = 'adam',
              loss = 'sparse_categorical_crossentropy')

model.fit(training_images, training_labels, epochs=5)

model.evaluate(test_images, test_labels)

classifications = model.predict(test_images)

print(classifications[0])
print(test_labels[0])
```

    2.2.0
    Epoch 1/5
    1875/1875 [==============================] - 4s 2ms/step - loss: 0.1850
    Epoch 2/5
    1875/1875 [==============================] - 4s 2ms/step - loss: 0.0808
    Epoch 3/5
    1875/1875 [==============================] - 4s 2ms/step - loss: 0.0550
    Epoch 4/5
    1875/1875 [==============================] - 4s 2ms/step - loss: 0.0412
    Epoch 5/5
    1875/1875 [==============================] - 4s 2ms/step - loss: 0.0330
    313/313 [==============================] - 0s 1ms/step - loss: 0.0769
    [8.0844446e-13 6.1690521e-08 6.4078776e-08 3.7278909e-07 2.6953151e-09
     5.7083088e-10 2.3449347e-13 9.9999404e-01 5.7558780e-10 5.3824215e-06]
    7

### 练习6

请考虑改变训练epochs次数，为有什么影响？

试试15个epochs--可能会得到一个比5个epochs更好的模型,损失更小。
试试30个epochs--可能会看到损失值停止下降，有时还会增加。这是被称为 "过拟合 "的副作用，在训练神经网络时，需要一直注意这个问题。如果损失没有改善，那么浪费时间继续训练是没有意义的，对吧！ :)


```python
import tensorflow as tf
print(tf.__version__)

mnist = tf.keras.datasets.mnist

(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()

training_images = training_images/255.0
test_images = test_images/255.0

model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),
                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

model.compile(optimizer = 'adam',
              loss = 'sparse_categorical_crossentropy')

model.fit(training_images, training_labels, epochs=30)

model.evaluate(test_images, test_labels)

classifications = model.predict(test_images)

print(classifications[34])
print(test_labels[34])
```

    2.2.0
    Epoch 1/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.2595
    Epoch 2/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.1162
    Epoch 3/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0798
    Epoch 4/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0582
    Epoch 5/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0460
    Epoch 6/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0353
    Epoch 7/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0284
    Epoch 8/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0223
    Epoch 9/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0201
    Epoch 10/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0155
    Epoch 11/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0147
    Epoch 12/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0117
    Epoch 13/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0104
    Epoch 14/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0090
    Epoch 15/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0086
    Epoch 16/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0090
    Epoch 17/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0061
    Epoch 18/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0068
    Epoch 19/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0053
    Epoch 20/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0062
    Epoch 21/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0054
    Epoch 22/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0053
    Epoch 23/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0059
    Epoch 24/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0047
    Epoch 25/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0055
    Epoch 26/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0044
    Epoch 27/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0043
    Epoch 28/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0034
    Epoch 29/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0036
    Epoch 30/30
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0045
    313/313 [==============================] - 0s 830us/step - loss: 0.1233
    [2.1367544e-24 5.0140433e-17 2.2283821e-14 2.6118363e-13 9.7950895e-34
     2.3054374e-29 0.0000000e+00 1.0000000e+00 4.8939686e-18 7.3670806e-21]
    7

### 练习7

在训练之前，对数据进行了归一化处理，从0-255的数值变成了0-1的数值。去掉归一化会有什么影响？下面是完整的代码，可以试一试。会得到什么不同的结果？


```python
import tensorflow as tf
print(tf.__version__)
mnist = tf.keras.datasets.mnist
(training_images, training_labels), (test_images, test_labels) = mnist.load_data()
training_images=training_images/255.0
test_images=test_images/255.0
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(512, activation=tf.nn.relu),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
model.fit(training_images, training_labels, epochs=5)
model.evaluate(test_images, test_labels)
classifications = model.predict(test_images)
print(classifications[0])
print(test_labels[0])
```

    2.2.0
    Epoch 1/5
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.2020
    Epoch 2/5
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.0793
    Epoch 3/5
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.0528
    Epoch 4/5
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.0355
    Epoch 5/5
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.0281
    313/313 [==============================] - 0s 1ms/step - loss: 0.0595
    [6.4718949e-07 4.8794635e-09 2.1555893e-06 1.3272098e-04 8.9761018e-11
     9.7716210e-08 4.4795799e-12 9.9963272e-01 2.2070760e-06 2.2938165e-04]
    7

### 练习8

之前在训练模型的时候，你可能会想'如果可以在达到一个期望值的时候停止训练不是很好吗？'--即95%的准确率对你来说可能已经足够了，如果你在3个epochs后达到了这个值，为什么还要坐等它完成更多的训练次数呢....，那么如何解决这个问题？就像其他很多类似程序一样......运用回调函数！让我们来看看它的实际作用。


```python
import tensorflow as tf
print(tf.__version__)

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('loss')<0.4):
      print("\nReached 60% accuracy so cancelling training!")
      self.model.stop_training = True

callbacks = myCallback()
mnist = tf.keras.datasets.fashion_mnist
(training_images, training_labels), (test_images, test_labels) = mnist.load_data()
training_images=training_images/255.0
test_images=test_images/255.0
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(512, activation=tf.nn.relu),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
model.fit(training_images, training_labels, epochs=5, callbacks=[callbacks])



```

    2.2.0
    Epoch 1/5
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.4726
    Epoch 2/5
    1864/1875 [============================>.] - ETA: 0s - loss: 0.3590
    Reached 60% accuracy so cancelling training!
    1875/1875 [==============================] - 3s 2ms/step - loss: 0.3588

## 练习(MNIST)

在课程中，已经学习了如何用Fashion MNIST数据集训练分类器。Fashion MNIST是一个包含服装项目的数据集。还有另一个类似的数据集叫MNIST，它包含很多手写数字（0到9）的图片和标签。

编写一个MNIST分类器，让它可以训练到99%以上的准确率，并且在达到这个准确率时，就通过回调函数停止训练。

一些注意事项。
1. 它应该在小于10个epochs的情况下成功，所以把epochs改成10个也可以，但不能大过10个。
2. 当它达到99%以上时，应该打印出 "达到99%准确率，所以取消训练！"的字符串。
3. 如果你添加了任何额外的变量，请确保你使用与类中使用的变量相同的名称。

我已经为你准备了下面的代码--请完成这个程序。


```python
# YOUR CODE SHOULD START HERE
# YOUR CODE SHOULD END HERE
import tensorflow as tf
mnist = tf.keras.datasets.mnist

(x_train, y_train),(x_test, y_test) = mnist.load_data()
# YOUR CODE SHOULD START HERE

# YOUR CODE SHOULD END HERE
model = tf.keras.models.Sequential([
# YOUR CODE SHOULD START HERE
    
# YOUR CODE SHOULD END HERE
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# YOUR CODE SHOULD START HERE
# YOUR CODE SHOULD END HERE
```


```python
%config IPCompleter.greedy=True
```


```python
# YOUR CODE SHOULD START HERE
# YOUR CODE SHOULD END HERE
import tensorflow as tf
mnist = tf.keras.datasets.mnist

class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self,epoch,logs={}):
        if(logs.get('accuracy')>0.99):
            print("\nReached 99% accuracy so cancelling training!")
            self.model.stop_training=True
# class myCallback(tf.keras.callbacks.Callback):
#   def on_epoch_end(self, epoch, logs={}):
#     if(logs.get('accuracy')>0.99):
#       print("\nReached 99% accuracy so cancelling training!")
#       self.model.stop_training = True




(x_train, y_train),(x_test, y_test) = mnist.load_data()
# YOUR CODE SHOULD START HERE
x_train_s=x_train/255.0
x_test_s=x_test/255.0

# YOUR CODE SHOULD END HERE
callbacks=myCallback()
# model = tf.keras.models.Sequential([
# # YOUR CODE SHOULD START HERE


# # YOUR CODE SHOULD END HERE
# ])
model=tf.keras.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(28,28)))
model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(10,activation=tf.nn.softmax))
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.fit(x_train_s,y_train,epochs=10,callbacks=callbacks)
# YOUR CODE SHOULD START HERE
# YOUR CODE SHOULD END HERE
```

    Epoch 1/10
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.2613 - accuracy: 0.9255
    Epoch 2/10
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.1142 - accuracy: 0.9661
    Epoch 3/10
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0787 - accuracy: 0.9765
    Epoch 4/10
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0573 - accuracy: 0.9818
    Epoch 5/10
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0453 - accuracy: 0.9857
    Epoch 6/10
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0353 - accuracy: 0.9892
    Epoch 7/10
    1872/1875 [============================>.] - ETA: 0s - loss: 0.0282 - accuracy: 0.9909
    Reached 99% accuracy so cancelling training!
    1875/1875 [==============================] - 2s 1ms/step - loss: 0.0282 - accuracy: 0.9909





    <tensorflow.python.keras.callbacks.History at 0x7f8ba64cc150>




```python
# YOUR CODE SHOULD START HERE
# YOUR CODE SHOULD END HERE
import tensorflow as tf
mnist = tf.keras.datasets.mnist

class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self,epoch,logs={}):
        if(logs.get('accuracy')>0.999):
            print("\nReached 99% accuracy so cancelling training!")
            self.model.stop_training=True
# class myCallback(tf.keras.callbacks.Callback):
#   def on_epoch_end(self, epoch, logs={}):
#     if(logs.get('accuracy')>0.99):
#       print("\nReached 99% accuracy so cancelling training!")
#       self.model.stop_training = True




(x_train, y_train),(x_test, y_test) = mnist.load_data()
# YOUR CODE SHOULD START HERE
x_train_s=x_train/255.0
x_test_s=x_test/255.0

# YOUR CODE SHOULD END HERE
callbacks=myCallback()
# model = tf.keras.models.Sequential([
# # YOUR CODE SHOULD START HERE


# # YOUR CODE SHOULD END HERE
# ])
model=tf.keras.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(28,28)))
model.add(tf.keras.layers.Dense(256,activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(10,activation=tf.nn.softmax))
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.fit(x_train_s,y_train,epochs=10,callbacks=callbacks)
# YOUR CODE SHOULD START HERE
# YOUR CODE SHOULD END HERE
```

    Epoch 1/10
    1875/1875 [==============================] - 3s 1ms/step - loss: 0.2240 - accuracy: 0.9347
    Epoch 2/10
    1875/1875 [==============================] - 3s 1ms/step - loss: 0.0916 - accuracy: 0.9726
    Epoch 3/10
    1875/1875 [==============================] - 3s 1ms/step - loss: 0.0600 - accuracy: 0.9820
    Epoch 4/10
    1875/1875 [==============================] - 3s 1ms/step - loss: 0.0438 - accuracy: 0.9869
    Epoch 5/10
    1875/1875 [==============================] - 3s 1ms/step - loss: 0.0318 - accuracy: 0.9905
    Epoch 6/10
    1875/1875 [==============================] - 3s 1ms/step - loss: 0.0258 - accuracy: 0.9916
    Epoch 7/10
    1875/1875 [==============================] - 3s 1ms/step - loss: 0.0193 - accuracy: 0.9937
    Epoch 8/10
    1875/1875 [==============================] - 3s 1ms/step - loss: 0.0159 - accuracy: 0.9948
    Epoch 9/10
    1875/1875 [==============================] - 3s 1ms/step - loss: 0.0131 - accuracy: 0.9960
    Epoch 10/10
    1875/1875 [==============================] - 3s 1ms/step - loss: 0.0110 - accuracy: 0.9964

